{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5e2a076",
   "metadata": {},
   "source": [
    "# GDPR Compliance Assistant - RAG Agent Implementation\n",
    "\n",
    "This notebook implements the QA agent for the GDPR Compliance Assistant using your existing Pinecone vector database.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3624fb25",
   "metadata": {},
   "source": [
    "\n",
    "## Setup and Imports\n",
    "\n",
    "First, let's install required packages and import dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "063b3123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ All packages imported successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/guillermo/venvs/langchain_venv/lib/python3.11/site-packages/langchain_pinecone/__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
      "\n",
      "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
      "with: `from pydantic import BaseModel`\n",
      "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
      "\n",
      "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
     ]
    }
   ],
   "source": [
    "# First, make sure you have the latest LangChain\n",
    "# pip install langchain-core langchain-openai\n",
    "\n",
    "# Cell 1: Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Add project root to Python path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# LangChain components\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI  # ‚úÖ Correct imports\n",
    "from langchain_pinecone import PineconeVectorStore  # ‚úÖ Pinecone integration\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# from langchain.chains import RetrievalQA\n",
    "# from langchain.vectorstores import Pinecone\n",
    "# from langchain.embeddings import OpenAIEmbeddings\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain.prompts import PromptTemplate\n",
    "# from langchain_pinecone import PineconeVectorStore\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"‚úÖ All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e633dbb",
   "metadata": {},
   "source": [
    "\n",
    "## Configuration / Environment Setup\n",
    "\n",
    "Set up your API keys and configuration. Replace with your actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3f43176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîë API keys configured\n",
      "üìÅ Using Pinecone index: gdpr-compliance-openai\n"
     ]
    }
   ],
   "source": [
    "# Configure your API keys\n",
    "def setup_environment():\n",
    "    # Check if API keys are already in environment\n",
    "    openai_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "    pinecone_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "    \n",
    "    # If not set, prompt user\n",
    "    if not openai_key:\n",
    "        openai_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "        os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "    \n",
    "    if not pinecone_key:\n",
    "        pinecone_key = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "        os.environ[\"PINECONE_API_KEY\"] = pinecone_key\n",
    "    \n",
    "    # Your Pinecone index name (replace with your actual index name)\n",
    "    index_name = \"gdpr-compliance-openai\"  # Change this to your index name\n",
    "    \n",
    "    return index_name\n",
    "\n",
    "index_name = setup_environment()\n",
    "print(f\"üîë API keys configured\")\n",
    "print(f\"üìÅ Using Pinecone index: {index_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b052bb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Initializing Pinecone...\n",
      "‚úÖ Pinecone initialized successfully\n",
      "‚úÖ Index 'gdpr-compliance-openai' exists\n",
      "‚úÖ Pinecone setup completed!\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------\n",
    "# Pinecone Initialization (Current 2025 syntax)\n",
    "# ---------------------------\n",
    "def init_pinecone(api_key: str, index_name: str = \"gdpr-assistant\", environment: str = \"us-east-1\"):\n",
    "    \"\"\"\n",
    "    Initialize Pinecone connection using current Pinecone v7.x+ API\n",
    "    \"\"\"\n",
    "    if not api_key:\n",
    "        raise ValueError(\"PINECONE_API_KEY is missing!\")\n",
    "    \n",
    "    # Initialize Pinecone (Current API)\n",
    "    print(\"üîå Initializing Pinecone...\")\n",
    "    from pinecone import Pinecone, ServerlessSpec\n",
    "    pc = Pinecone(api_key=api_key)\n",
    "    print(\"‚úÖ Pinecone initialized successfully\")\n",
    "    \n",
    "    # Check if index exists\n",
    "    if index_name in pc.list_indexes().names():\n",
    "        print(f\"‚úÖ Index '{index_name}' exists\")\n",
    "        # Wait for index to be ready\n",
    "        while not pc.describe_index(index_name).status.ready:\n",
    "            print(\"‚è≥ Waiting for index to be ready...\")\n",
    "            import time\n",
    "            time.sleep(1)\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  Index '{index_name}' not found. Creating it...\")\n",
    "        pc.create_index(\n",
    "            name=index_name,\n",
    "            dimension=1536,  # OpenAI text-embedding-3-small dimension\n",
    "            metric=\"cosine\",\n",
    "            spec=ServerlessSpec(cloud=\"aws\", region=environment)\n",
    "        )\n",
    "        print(f\"‚úÖ Index '{index_name}' created\")\n",
    "    \n",
    "    # Get the index object\n",
    "    index = pc.Index(index_name)\n",
    "    return pc, index\n",
    "\n",
    "# Initialize Pinecone with your settings\n",
    "try:\n",
    "    pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "    pc, index = init_pinecone(\n",
    "        api_key=pinecone_api_key,\n",
    "        index_name=index_name,\n",
    "        environment=\"us-east-1\"\n",
    "    )\n",
    "    print(\"‚úÖ Pinecone setup completed!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error initializing Pinecone: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2133cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîå Initializing Pinecone...\n",
      "‚úÖ Pinecone initialized successfully\n",
      "‚úÖ Index 'gdpr-compliance-openai' exists\n",
      "‚úÖ Pinecone setup completed!\n"
     ]
    }
   ],
   "source": [
    "# # ---------------------------\n",
    "# # Pinecone Initialization (Adapted from colleague's code)\n",
    "# # ---------------------------\n",
    "# def init_pinecone(api_key: str, index_name: str = \"gdpr-assistant\", environment: str = \"us-east-1\"):\n",
    "#     \"\"\"\n",
    "#     Initialize Pinecone connection using the new Pinecone v7.x API\n",
    "#     Adapted from colleague's research-papers project\n",
    "#     \"\"\"\n",
    "#     if not api_key:\n",
    "#         raise ValueError(\"PINECONE_API_KEY is missing!\")\n",
    "    \n",
    "#     # Initialize Pinecone (NEW API - Version 7.x)\n",
    "#     print(\"üîå Initializing Pinecone...\")\n",
    "#     from pinecone import Pinecone\n",
    "#     pc = Pinecone(api_key=api_key)\n",
    "#     print(\"‚úÖ Pinecone initialized successfully\")\n",
    "    \n",
    "#     # Check if index exists\n",
    "#     if index_name in pc.list_indexes().names():\n",
    "#         print(f\"‚úÖ Index '{index_name}' exists\")\n",
    "#         # Wait for index to be ready\n",
    "#         while not pc.describe_index(index_name).status.ready:\n",
    "#             print(\"‚è≥ Waiting for index to be ready...\")\n",
    "#             import time\n",
    "#             time.sleep(1)\n",
    "#     else:\n",
    "#         print(f\"‚ö†Ô∏è  Index '{index_name}' not found. Creating it...\")\n",
    "#         pc.create_index(\n",
    "#             name=index_name,\n",
    "#             dimension=1536,  # OpenAI text-embedding-3-small dimension\n",
    "#             metric=\"cosine\",\n",
    "#             spec=ServerlessSpec(cloud=\"aws\", region=environment)\n",
    "#         )\n",
    "#         print(f\"‚úÖ Index '{index_name}' created\")\n",
    "    \n",
    "#     # Get the index object\n",
    "#     index = pc.Index(index_name)\n",
    "#     return pc, index  # Return both client and index\n",
    "\n",
    "# # Initialize Pinecone with your settings\n",
    "# try:\n",
    "#     pinecone_api_key = os.getenv(\"PINECONE_API_KEY\")\n",
    "#     pc, index = init_pinecone(\n",
    "#         api_key=pinecone_api_key,\n",
    "#         index_name=index_name,\n",
    "#         environment=\"us-east-1\"  # Adjust if your index is in different region\n",
    "#     )\n",
    "#     print(\"‚úÖ Pinecone setup completed!\")\n",
    "    \n",
    "# except Exception as e:\n",
    "#     print(f\"‚ùå Error initializing Pinecone: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa568443",
   "metadata": {},
   "source": [
    "## Initialize embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9fbff3dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embeddings initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize embeddings with CURRENT syntax - NO DEPRECATION WARNING\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "print(\"‚úÖ Embeddings initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26256a4e",
   "metadata": {},
   "source": [
    "## Initialize Vector Store Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c077e1c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gdpr-compliance-openai'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b5dc5a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ LangChain successfully connected to Pinecone index!\n"
     ]
    }
   ],
   "source": [
    "vector_store = PineconeVectorStore(\n",
    "        index=index,  # Use the index object from our initialization\n",
    "        embedding=embeddings,\n",
    "        text_key=\"text\"  # This should match your upload metadata field name\n",
    "    )\n",
    "    \n",
    "print(\"‚úÖ LangChain successfully connected to Pinecone index!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ebddf7",
   "metadata": {},
   "source": [
    "## Test the connection with current syntax\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7db7f2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìö Test retrieval found 2 documents\n",
      "üìã Available metadata fields: ['author', 'chunk_id', 'chunk_size', 'content_category', 'content_length', 'creationdate', 'document_name', 'document_type', 'language', 'moddate', 'page', 'page_label', 'page_number', 'section_type', 'source', 'total_chunks', 'total_pages']\n",
      "üìÑ Sample content: Leitfaden \n",
      "Datenschutzrecht \n",
      "Was Betriebe zu beachten haben \n",
      " \n",
      " \n",
      "Stand: November 2020 \n",
      " \n",
      "Abteilung Organisation und Recht...\n",
      "\n",
      "üîç Vector store type: <class 'langchain_pinecone.vectorstores.PineconeVectorStore'>\n"
     ]
    }
   ],
   "source": [
    "# Test the connection with current syntax\n",
    "test_results = vector_store.similarity_search(\"Datenschutz\", k=2)\n",
    "print(f\"üìö Test retrieval found {len(test_results)} documents\")\n",
    "\n",
    "# Show metadata structure (useful for debugging)\n",
    "if test_results:\n",
    "    print(f\"üìã Available metadata fields: {list(test_results[0].metadata.keys())}\")\n",
    "    print(f\"üìÑ Sample content: {test_results[0].page_content[:150]}...\")\n",
    "    \n",
    "# Alternative: Check what's in the vector store\n",
    "print(f\"\\nüîç Vector store type: {type(vector_store)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62038f3b",
   "metadata": {},
   "source": [
    "## Verify Data and Create Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f35af9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Setting up retriever...\n",
      "‚úÖ Retriever configured!\n",
      "   - Search type: similarity\n",
      "   - k: 3 documents\n",
      "   - score_threshold: 0.7\n",
      "\n",
      "üß™ Testing retriever...\n",
      "‚úÖ Retriever test successful - found 3 documents\n"
     ]
    }
   ],
   "source": [
    "# Verify data and create retriever with current syntax\n",
    "print(\"üîç Setting up retriever...\")\n",
    "\n",
    "# Create retriever with current syntax\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\n",
    "        \"k\": 3,  # Number of documents to retrieve\n",
    "        \"score_threshold\": 0.7  # Optional: minimum similarity score\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Retriever configured!\")\n",
    "print(f\"   - Search type: similarity\")\n",
    "print(f\"   - k: 3 documents\")\n",
    "print(f\"   - score_threshold: 0.7\")\n",
    "\n",
    "# Test the retriever\n",
    "print(\"\\nüß™ Testing retriever...\")\n",
    "test_docs = retriever.invoke(\"Datenverarbeitung Grunds√§tze\")\n",
    "print(f\"‚úÖ Retriever test successful - found {len(test_docs)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f025b9f",
   "metadata": {},
   "source": [
    "## Current LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "011814e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Initializing GPT-5 Nano LLM...\n",
      "‚úÖ LLM initialized with current syntax!\n",
      "   - Model: gpt-5-nano\n",
      "   - Temperature: 0.1\n",
      "   - Max tokens: 500\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM with current syntax\n",
    "print(\"üöÄ Initializing GPT-5 Nano LLM...\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    temperature=0.1,\n",
    "    max_tokens=500,\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "print(\"‚úÖ LLM initialized with current syntax!\")\n",
    "print(f\"   - Model: gpt-5-nano\")\n",
    "print(f\"   - Temperature: 0.1\") \n",
    "print(f\"   - Max tokens: 500\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dde5c2",
   "metadata": {},
   "source": [
    "## Create QA Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6db88797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó Creating QA chain...\n",
      "‚úÖ QA chain created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create prompt template and QA chain with current syntax\n",
    "print(\"üîó Creating QA chain...\")\n",
    "\n",
    "# Current prompt template\n",
    "prompt_template = \"\"\"Du bist ein spezialisierter Assistent f√ºr Datenschutzfragen f√ºr Handwerksbetriebe\n",
    ".\n",
    "\n",
    "Antworte AUF DEUTSCH basierend auf dem bereitgestellten Kontext. \n",
    "Sei pr√§zise und fokussiere auf die praktische Umsetzung f√ºr Handwerksbetriebe.\n",
    "\n",
    "Kontext: {context}\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Antwort (deutsch, pr√§zise, praxisorientiert):\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create QA chain with current syntax\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"‚úÖ QA chain created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1cabd84",
   "metadata": {},
   "source": [
    "## Create a helper function to test the agent and display results.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "57d421ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_gdpr_question(question, show_sources=True):\n",
    "    \"\"\"\n",
    "    Ask a question to the GDPR assistant and display the response with sources.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to ask (in German or English)\n",
    "        show_sources (bool): Whether to display source documents\n",
    "    \n",
    "    Returns:\n",
    "        dict: Complete result with answer and source documents\n",
    "    \"\"\"\n",
    "    print(f\"‚ùì Frage: {question}\")\n",
    "    print(\"‚è≥ Denke nach...\")\n",
    "    \n",
    "    # Get answer from QA chain\n",
    "    result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "    # Check if we got a valid answer\n",
    "    answer = result.get('result', '').strip()\n",
    "    \n",
    "    print(f\"‚úÖ Antwort: {result['result']}\")\n",
    "    \n",
    "    # Show source documents if requested\n",
    "    if show_sources and result['source_documents']:\n",
    "        print(f\"\\nüìö Verwendete Quellen ({len(result['source_documents'])}):\")\n",
    "        for i, doc in enumerate(result['source_documents']):\n",
    "            source_text = doc.page_content.replace('\\n', ' ').strip()\n",
    "            print(f\"   {i+1}. {source_text[:150]}...\")\n",
    "    \n",
    "    print(\"‚Äï\" * 80)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c2f969",
   "metadata": {},
   "source": [
    "## Test the RAG System\n",
    "\n",
    "Now let's test the system with various GDPR questions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "777fa30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ TEST 2: Aufbewahrungsfristen\n",
      "‚ùì Frage: Wie lange d√ºrfen Kundendaten gespeichert werden?\n",
      "‚è≥ Denke nach...\n",
      "‚úÖ Antwort: \n",
      "\n",
      "üìö Verwendete Quellen (3):\n",
      "   1. Gesetzliche L√∂schfristen     In vereinzelten F√§llen schreiben gesetzliche Regelungen vor, wann bestimmte Daten zu l√∂- schen sind (f√ºr eine √ú bersicht ...\n",
      "   2. Ob und wann die Aufbewahrung von Daten nicht mehr erforderlich ist, liegt grunds√§tzlich im  Ermessen des Dateninhabers, also des Handwerksbetriebs, de...\n",
      "   3. ben√∂tigt, schreiben zahlreichliche gesetzliche Regelungen vor, dass bestimmte Daten min- destens f√ºr einen konkreten Zeitraum aufzubewahren sind. Solc...\n",
      "‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï‚Äï\n"
     ]
    }
   ],
   "source": [
    "# Test 2: Data retention periods\n",
    "print(\"üß™ TEST 2: Aufbewahrungsfristen\")\n",
    "result2 = ask_gdpr_question(\"Wie lange d√ºrfen Kundendaten gespeichert werden?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8fd2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test 1: Basic GDPR principles\n",
    "print(\"üß™ TEST 1: Grundlegende Datenschutzgrunds√§tze\")\n",
    "result1 = ask_gdpr_question(\"Was sind die Grunds√§tze der Datenverarbeitung im Handwerk?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8f9fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "zzz\n",
    "# Test 3: Data breach procedures\n",
    "print(\"üß™ TEST 3: Datenpannen\")\n",
    "result3 = ask_gdpr_question(\"Was muss ich tun bei einer Datenschutzverletzung?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4fc1467",
   "metadata": {},
   "outputs": [],
   "source": [
    "zz\n",
    "# Test 4: Employee data\n",
    "print(\"üß™ TEST 4: Mitarbeiterdaten\")\n",
    "result4 = ask_gdpr_question(\"Welche Regeln gelten f√ºr die Verarbeitung von Mitarbeiterdaten?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d148b49",
   "metadata": {},
   "source": [
    "===\n",
    "----\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efce7ff6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c98b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7088cfc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "```python\n",
    "# Install required packages (run once)\n",
    "# !pip install langchain langchain-community langchain-openai python-dotenv pinecone-client\n",
    "```\n",
    "\n",
    "```python\n",
    "# Import dependencies\n",
    "import os\n",
    "import getpass\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.vectorstores import Pinecone\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# For environment variables\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "```\n",
    "\n",
    "## Configuration\n",
    "\n",
    "Set up your API keys and configuration. Replace with your actual values.\n",
    "\n",
    "```python\n",
    "# Configure API keys\n",
    "if \"OPENAI_API_KEY\" not in os.environ:\n",
    "    openai_key = getpass.getpass(\"Enter your OpenAI API key: \")\n",
    "    os.environ[\"OPENAI_API_KEY\"] = openai_key\n",
    "\n",
    "if \"PINECONE_API_KEY\" not in os.environ:\n",
    "    pinecone_key = getpass.getpass(\"Enter your Pinecone API key: \")\n",
    "    os.environ[\"PINECONE_API_KEY\"] = pinecone_key\n",
    "\n",
    "# Your Pinecone index name (replace with your actual index name)\n",
    "INDEX_NAME = \"gdpr-assistant\"  # Change this to your index name\n",
    "\n",
    "print(\"‚úÖ API keys configured\")\n",
    "print(f\"üìÅ Using Pinecone index: {INDEX_NAME}\")\n",
    "```\n",
    "\n",
    "## Initialize Components\n",
    "\n",
    "Set up the embeddings, vector store connection, and retriever.\n",
    "\n",
    "```python\n",
    "# 1. Initialize embeddings (same as used for Pinecone upload)\n",
    "print(\"üîÑ Initializing embeddings...\")\n",
    "embeddings = OpenAIEmbeddings(\n",
    "    model=\"text-embedding-3-small\",\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# 2. Connect to existing Pinecone index\n",
    "print(\"üîó Connecting to Pinecone...\")\n",
    "vector_store = Pinecone.from_existing_index(\n",
    "    index_name=INDEX_NAME,\n",
    "    embedding=embeddings\n",
    ")\n",
    "\n",
    "# 3. Set up retriever\n",
    "print(\"üéØ Setting up retriever...\")\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}  # Retrieve top 3 most relevant chunks\n",
    ")\n",
    "\n",
    "print(\"‚úÖ All components initialized successfully!\")\n",
    "```\n",
    "\n",
    "## Create Custom Prompt\n",
    "\n",
    "Design a German-language prompt optimized for GDPR questions and GPT-5 Nano.\n",
    "\n",
    "```python\n",
    "# Create optimized prompt for German GDPR questions\n",
    "prompt_template = \"\"\"Du bist ein spezialisierter Assistent f√ºr Datenschutzfragen im Handwerk. \n",
    "Beantworte die Frage basierend ausschlie√ülich auf dem bereitgestellten Kontext. \n",
    "Sei pr√§zise und sachlich.\n",
    "\n",
    "Kontext: {context}\n",
    "\n",
    "Frage: {question}\n",
    "\n",
    "Antwort (auf Deutsch, kurz und fachlich):\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template, \n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "print(\"üìù Prompt template created:\")\n",
    "print(PROMPT.template)\n",
    "```\n",
    "\n",
    "## Initialize QA Agent\n",
    "\n",
    "Set up the GPT-5 Nano model and create the retrieval QA chain.\n",
    "\n",
    "```python\n",
    "# Initialize GPT-5 Nano model\n",
    "print(\"ü§ñ Initializing GPT-5 Nano...\")\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-5-nano\",\n",
    "    temperature=0.1,      # Low temperature for factual answers\n",
    "    max_tokens=500,       # Limit response length\n",
    "    openai_api_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Create the QA chain\n",
    "print(\"üîó Creating QA chain...\")\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",   # Simple and efficient\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    return_source_documents=True  # To see which documents were used\n",
    ")\n",
    "\n",
    "print(\"‚úÖ QA agent created successfully!\")\n",
    "```\n",
    "\n",
    "## Test Function\n",
    "\n",
    "Create a helper function to test the agent and display results.\n",
    "\n",
    "```python\n",
    "def ask_gdpr_question(question, show_sources=True):\n",
    "    \"\"\"\n",
    "    Ask a question to the GDPR assistant and display the response with sources.\n",
    "    \n",
    "    Args:\n",
    "        question (str): The question to ask (in German or English)\n",
    "        show_sources (bool): Whether to display source documents\n",
    "    \n",
    "    Returns:\n",
    "        dict: Complete result with answer and source documents\n",
    "    \"\"\"\n",
    "    print(f\"‚ùì Frage: {question}\")\n",
    "    print(\"‚è≥ Denke nach...\")\n",
    "    \n",
    "    # Get answer from QA chain\n",
    "    result = qa_chain({\"query\": question})\n",
    "    \n",
    "    print(f\"‚úÖ Antwort: {result['result']}\")\n",
    "    \n",
    "    # Show source documents if requested\n",
    "    if show_sources and result['source_documents']:\n",
    "        print(f\"\\nüìö Verwendete Quellen ({len(result['source_documents'])}):\")\n",
    "        for i, doc in enumerate(result['source_documents']):\n",
    "            source_text = doc.page_content.replace('\\n', ' ').strip()\n",
    "            print(f\"   {i+1}. {source_text[:150]}...\")\n",
    "    \n",
    "    print(\"‚Äï\" * 80)\n",
    "    return result\n",
    "```\n",
    "\n",
    "## Test the RAG System\n",
    "\n",
    "Now let's test the system with various GDPR questions.\n",
    "\n",
    "```python\n",
    "# Test 1: Basic GDPR principles\n",
    "print(\"üß™ TEST 1: Grundlegende Datenschutzgrunds√§tze\")\n",
    "result1 = ask_gdpr_question(\"Was sind die Grunds√§tze der Datenverarbeitung im Handwerk?\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Test 2: Data retention periods\n",
    "print(\"üß™ TEST 2: Aufbewahrungsfristen\")\n",
    "result2 = ask_gdpr_question(\"Wie lange d√ºrfen Kundendaten gespeichert werden?\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Test 3: Data breach procedures\n",
    "print(\"üß™ TEST 3: Datenpannen\")\n",
    "result3 = ask_gdpr_question(\"Was muss ich tun bei einer Datenschutzverletzung?\")\n",
    "```\n",
    "\n",
    "```python\n",
    "# Test 4: Employee data\n",
    "print(\"üß™ TEST 4: Mitarbeiterdaten\")\n",
    "result4 = ask_gdpr_question(\"Welche Regeln gelten f√ºr die Verarbeitung von Mitarbeiterdaten?\")\n",
    "```\n",
    "\n",
    "## Advanced Testing\n",
    "\n",
    "Test with more specific scenarios to evaluate the system's performance.\n",
    "\n",
    "```python\n",
    "# Test with more specific scenarios\n",
    "specific_tests = [\n",
    "    \"Muss ich f√ºr Marketing-E-Mails immer eine Einwilligung haben?\",\n",
    "    \"Was ist eine Datenschutzfolgenabsch√§tzung und wann ist sie erforderlich?\",\n",
    "    \"Darf ich Fotos von meinen Handwerksarbeiten auf der Website verwenden?\",\n",
    "    \"Wie behandle ich Daten von Lieferanten und Partnern?\",\n",
    "]\n",
    "\n",
    "print(\"üß™ SPEZIFISCHE TESTS\")\n",
    "for i, question in enumerate(specific_tests, 1):\n",
    "    print(f\"\\nTest {i}/4:\")\n",
    "    ask_gdpr_question(question, show_sources=True)\n",
    "```\n",
    "\n",
    "## Verify Source Quality\n",
    "\n",
    "Check if the retrieved documents are relevant to the questions.\n",
    "\n",
    "```python\n",
    "def analyze_source_relevance(question, top_k=5):\n",
    "    \"\"\"\n",
    "    Analyze which documents are being retrieved for a question\n",
    "    \"\"\"\n",
    "    print(f\"üîç Analyzing sources for: '{question}'\")\n",
    "    \n",
    "    # Get documents directly from retriever\n",
    "    docs = retriever.get_relevant_documents(question)\n",
    "    \n",
    "    print(f\"üìÑ Retrieved {len(docs)} documents:\")\n",
    "    for i, doc in enumerate(docs):\n",
    "        print(f\"\\n--- Document {i+1} ---\")\n",
    "        content_preview = doc.page_content.replace('\\n', ' ').strip()\n",
    "        print(f\"Content: {content_preview[:200]}...\")\n",
    "        if hasattr(doc, 'metadata'):\n",
    "            print(f\"Metadata: {doc.metadata}\")\n",
    "\n",
    "# Test source relevance\n",
    "analyze_source_relevance(\"Datenaufbewahrung Kunden\")\n",
    "```\n",
    "\n",
    "## Save the QA Agent\n",
    "\n",
    "Save the configured agent for use in other notebooks or applications.\n",
    "\n",
    "```python\n",
    "# Function to quickly recreate the QA agent\n",
    "def get_qa_agent():\n",
    "    \"\"\"\n",
    "    Returns a pre-configured QA agent for GDPR questions\n",
    "    \"\"\"\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    vector_store = Pinecone.from_existing_index(INDEX_NAME, embeddings)\n",
    "    retriever = vector_store.as_retriever(search_kwargs={\"k\": 3})\n",
    "    \n",
    "    llm = ChatOpenAI(\n",
    "        model=\"gpt-5-nano\",\n",
    "        temperature=0.1,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    \n",
    "    return RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        chain_type=\"stuff\",\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT},\n",
    "        return_source_documents=True\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ QA agent function saved - ready for POC 1 completion!\")\n",
    "```\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Your POC 1 is now complete! Here's what you've accomplished:\n",
    "\n",
    "‚úÖ **POC 1 Completed**: Ask questions in German, receive answers in German based on ZDH guidelines\n",
    "\n",
    "**Next steps for POC 2** (Multilingual support):\n",
    "1. Modify the prompt to detect input language\n",
    "2. Add language switching logic\n",
    "3. Test with English questions\n",
    "\n",
    "**Quick test to verify everything works:**\n",
    "\n",
    "```python\n",
    "# Final verification\n",
    "final_test = ask_gdpr_question(\"Was sind meine Pflichten als Handwerksbetrieb bez√ºglich Datenschutz?\")\n",
    "print(\"üéâ POC 1 successfully implemented!\")\n",
    "```\n",
    "\n",
    "This notebook gives you a complete, working RAG system for your GDPR Compliance Assistant. Each cell can be run independently, and the entire system is now ready for your POC 1 demonstration!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d46a332e",
   "metadata": {},
   "source": [
    "-----\n",
    "----\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bb5e4d",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3435eb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat completion llm\n",
    "llm = ChatOpenAI(\n",
    "    openai_api_key=OPENAI_API_KEY,\n",
    "    model_name='gpt-3.5-turbo',\n",
    "    temperature=0.0\n",
    ")\n",
    "# conversational memory\n",
    "conversational_memory = ConversationBufferWindowMemory(\n",
    "    memory_key='chat_history',\n",
    "    k=5,\n",
    "    return_messages=True\n",
    ")\n",
    "# retrieval qa chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=vectorstore.as_retriever()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5bab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa.run(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6061a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "\n",
    "tools = [\n",
    "    Tool(\n",
    "        name='Knowledge Base',\n",
    "        func=qa.run,\n",
    "        description=(\n",
    "            'use this tool when answering general knowledge queries to get '\n",
    "            'more information about the topic'\n",
    "        )\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70be7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent\n",
    "\n",
    "agent = initialize_agent(\n",
    "    agent='chat-conversational-react-description',\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    verbose=True,\n",
    "    max_iterations=3,\n",
    "    early_stopping_method='generate',\n",
    "    memory=conversational_memory\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "langchain_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
