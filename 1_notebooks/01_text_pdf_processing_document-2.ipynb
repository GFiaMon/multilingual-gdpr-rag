{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46cda769",
   "metadata": {},
   "source": [
    "# üöÄ GDPR Compliance Agent - Notebook 1: PDF Processing\n",
    "\n",
    "## üìã Table of Contents\n",
    "1. [Project Overview](#project-overview)\n",
    "2. [Setup & Imports](#setup-imports)\n",
    "3. [Load & Explore Data PDF](#load-explore-data)\n",
    "4. [Text Chunking](#text-chunking)\n",
    "5. [Chunk Analysis](#chunk-analysis)\n",
    "6. [Save Results](#save-results)\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ Project Overview\n",
    "\n",
    "**Goal**: Create a GDPR compliance assistant that can answer questions about data protection guidelines.\n",
    "\n",
    "**This Notebook Focus**: Process text documents and prepare them for the vector database.\n",
    "\n",
    "**Key Steps**:\n",
    "- Load sample GDPR handbook\n",
    "- Extract text from German PDF\n",
    "- Split text into manageable chunks\n",
    "- Prepare for embedding generation\n",
    "\n",
    "---\n",
    "\n",
    "## ‚öôÔ∏è Setup & Imports\n",
    "\n",
    "*Import required libraries and set up the environment*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02793c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pypdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42fbbdce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Setup and Imports\n",
    "import os\n",
    "import sys\n",
    "# from dotenv import load_dotenv\n",
    "\n",
    "from langchain.document_loaders import TextLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "import tiktoken\n",
    "\n",
    "import pickle\n",
    "\n",
    "# Add the project root directory to Python path\n",
    "sys.path.append(os.path.abspath('..'))\n",
    "\n",
    "# Helper functions:\n",
    "from src.embedding_cost_calculator import calculate_embedding_cost, quick_cost\n",
    "\n",
    "# Load environment variables\n",
    "# load_dotenv()\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a17c830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Cost calculator test: 8 tokens = $0.00000016\n"
     ]
    }
   ],
   "source": [
    "# Quick test of our cost calculator\n",
    "test_text = \"This is a test of the cost calculator\"\n",
    "tokens, cost = quick_cost(test_text)\n",
    "print(f\"‚úÖ Cost calculator test: {tokens} tokens = ${cost:.8f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23f49f2",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6003c135",
   "metadata": {},
   "source": [
    "## üá©üá™ German PDF Extraction\n",
    "\n",
    "*Now let's extract text from your actual German PDF*\n",
    "\n",
    "**What we'll do**:\n",
    "1. Check if German PDF exists\n",
    "2. Extract text automatically\n",
    "3. Process German text chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46cf4124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: PDF Extraction with PyPDFLoader\n",
    "def extract_pdf_with_metadata(pdf_path):\n",
    "    \"\"\"Extract text from PDF using PyPDFLoader with enhanced metadata\"\"\"\n",
    "    try:\n",
    "        print(f\"üìÑ Extracting from: {pdf_path}\")\n",
    "        \n",
    "        if not os.path.exists(pdf_path):\n",
    "            print(f\"‚ùå File not found: {pdf_path}\")\n",
    "            print(\"üí° Please place your ZDH PDF in data/raw/ folder\")\n",
    "            return None\n",
    "        \n",
    "        # Use PyPDFLoader (faster and reliable)\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        print(f\"‚úÖ Successfully extracted {len(documents)} pages\")\n",
    "        \n",
    "        # Enhanced metadata for better retrieval\n",
    "        enhanced_docs = []\n",
    "        for i, doc in enumerate(documents):\n",
    "            # Define keys to remove from original PDF metadata\n",
    "            keys_to_remove = ['producer', 'creator']\n",
    "            \n",
    "            # Start with original metadata but remove unwanted keys\n",
    "            clean_metadata = {}\n",
    "            for key, value in doc.metadata.items():\n",
    "                if key not in keys_to_remove:\n",
    "                    clean_metadata[key] = value\n",
    "            \n",
    "            # Add our custom metadata\n",
    "            custom_metadata = {\n",
    "                \"document_type\": \"zdh_gdpr_handbook\",\n",
    "                \"document_name\": os.path.basename(pdf_path),\n",
    "                \"language\": \"german\",\n",
    "                \"source\": \"zdh_handbook\",\n",
    "                \"page_number\": i + 1,\n",
    "                \"total_pages\": len(documents),\n",
    "                \"content_length\": len(doc.page_content),\n",
    "                \"content_category\": categorize_content(doc.page_content),\n",
    "                \"section_type\": identify_section_type(doc.page_content),\n",
    "            }\n",
    "            \n",
    "            # Merge clean original metadata with our custom metadata\n",
    "            final_metadata = {**custom_metadata, **clean_metadata}\n",
    "            \n",
    "            enhanced_doc = Document(\n",
    "                page_content=doc.page_content,\n",
    "                metadata=final_metadata\n",
    "            )\n",
    "            enhanced_docs.append(enhanced_doc)\n",
    "        \n",
    "        # Show first page as sample\n",
    "        if enhanced_docs:\n",
    "            print(f\"\\nüìã First page sample:\")\n",
    "            print(enhanced_docs[0].page_content[:200] + \"...\")\n",
    "            print(f\"üìä Metadata: {enhanced_docs[0].metadata}\")\n",
    "        \n",
    "        return enhanced_docs\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå PDF extraction error: {e}\")\n",
    "        return None\n",
    "\n",
    "def categorize_content(text):\n",
    "    \"\"\"Categorize GDPR content for better filtering\"\"\"\n",
    "    text_lower = text.lower()\n",
    "    \n",
    "    if any(keyword in text_lower for keyword in ['kunde', 'customer', 'marketing']):\n",
    "        return \"customer_data\"\n",
    "    elif any(keyword in text_lower for keyword in ['mitarbeiter', 'employee', 'personal']):\n",
    "        return \"employee_data\"\n",
    "    elif any(keyword in text_lower for keyword in ['recht', 'law', 'gesetz', 'dsgvo']):\n",
    "        return \"legal_basis\"\n",
    "    elif any(keyword in text_lower for keyword in ['sicherheit', 'security', 'datenschutzverletzung']):\n",
    "        return \"security\"\n",
    "    elif any(keyword in text_lower for keyword in ['speicherung', 'retention', 'aufbewahrung']):\n",
    "        return \"data_retention\"\n",
    "    else:\n",
    "        return \"general\"\n",
    "\n",
    "def identify_section_type(text):\n",
    "    \"\"\"Identify section types for better chunking\"\"\"\n",
    "    text = text.strip()\n",
    "    if len(text) < 200 and any(indicator in text for indicator in ['KAPITEL', 'ARTIKEL', 'SECTION']):\n",
    "        return \"section_header\"\n",
    "    elif len(text) < 100 and text.isupper():\n",
    "        return \"heading\"\n",
    "    else:\n",
    "        return \"content\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebaaff61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÑ Extracting from: ../2_data/raw/ZDH_LEITFADEN_DATENSCHUTZ_BETRIEBE_HANDWERKER.pdf\n",
      "‚úÖ Successfully extracted 99 pages\n",
      "\n",
      "üìã First page sample:\n",
      "Leitfaden \n",
      "Datenschutzrecht \n",
      "Was Betriebe zu beachten haben \n",
      " \n",
      " \n",
      "Stand: November 2020 \n",
      " \n",
      "Abteilung Organisation und Recht...\n",
      "üìä Metadata: {'document_type': 'zdh_gdpr_handbook', 'document_name': 'ZDH_LEITFADEN_DATENSCHUTZ_BETRIEBE_HANDWERKER.pdf', 'language': 'german', 'source': '../2_data/raw/ZDH_LEITFADEN_DATENSCHUTZ_BETRIEBE_HANDWERKER.pdf', 'page_number': 1, 'total_pages': 99, 'content_length': 121, 'content_category': 'legal_basis', 'section_type': 'content', 'creationdate': '2020-11-06T11:24:59+01:00', 'author': 'Kasper, Lisa', 'moddate': '2020-11-06T11:24:59+01:00', 'page': 0, 'page_label': '1'}\n",
      "‚úÖ Ready to process 99 PDF pages\n",
      "üìä Embedding Cost Calculation\n",
      "   Model: text-embedding-3-small\n",
      "   Texts: 99\n",
      "   Total tokens: 46,778\n",
      "   Cost: $0.000936\n",
      "   Avg tokens per text: 472.5\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Extract German PDF\n",
    "german_pdf_path = \"../2_data/raw/ZDH_LEITFADEN_DATENSCHUTZ_BETRIEBE_HANDWERKER.pdf\"\n",
    "documents = extract_pdf_with_metadata(german_pdf_path)\n",
    "\n",
    "if not documents:\n",
    "    print(\"‚ùå No documents extracted. Stopping here.\")\n",
    "else:\n",
    "    print(f\"‚úÖ Ready to process {len(documents)} PDF pages\")\n",
    "    \n",
    "    # Estimate cost for raw PDF using our imported function\n",
    "    raw_texts = [doc.page_content for doc in documents]\n",
    "    cost_info = calculate_embedding_cost(raw_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4ae023",
   "metadata": {},
   "source": [
    "## ‚úÇÔ∏è Text Chunking\n",
    "\n",
    "*Split the document into smaller pieces for processing*\n",
    "\n",
    "**Why chunking matters**:\n",
    "- LLMs have context window limits\n",
    "- Smaller chunks are easier to search\n",
    "- Better precision in retrieval\n",
    "\n",
    "**Parameters we're using**:\n",
    "- `chunk_size=800`: Balance between context and precision\n",
    "- `chunk_overlap=120`: Maintain context between chunks\n",
    "- Smart separators: Prefer natural breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9840a0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üî® Processing text chunks...\n",
      "‚úÇÔ∏è Chunking Results:\n",
      "   Input documents: 99\n",
      "   Output chunks: 266\n",
      "   Chunk type: <class 'langchain_core.documents.base.Document'>\n",
      "üìä Chunk Size Analysis:\n",
      "   Average: 642 characters\n",
      "   Range: 7 - 799 characters\n",
      "\n",
      "üìã Chunk Samples:\n",
      "Chunk 1: Leitfaden \n",
      "Datenschutzrecht \n",
      "Was Betriebe zu beachten haben \n",
      " \n",
      " \n",
      "Stand: November 2020 \n",
      " \n",
      "Abteilung O...\n",
      "Size: 121 chars | Category: legal_basis\n",
      "---\n",
      "Chunk 2: Vorwort \n",
      "Seit dem 25. Mai 2018 gelten in allen Mitgliedstaaten der Europ√§ischen Union neue Daten-\n",
      "sc...\n",
      "Size: 749 chars | Category: legal_basis\n",
      "---\n",
      "Chunk 3: pekte und Fragen. Er bietet neben rechtlichen Erkl√§rungen zahlreiche Beispielsf√§lle, Checklis-\n",
      "ten u...\n",
      "Size: 644 chars | Category: legal_basis\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Text Chunking\n",
    "def create_optimized_splitter():\n",
    "    \"\"\"Create optimized splitter for GDPR legal documents\"\"\"\n",
    "    return RecursiveCharacterTextSplitter(\n",
    "        chunk_size=800,           # Optimal for legal text precision\n",
    "        chunk_overlap=120,        # 15% overlap for context\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"],\n",
    "        length_function=len,      # Measures chunk size in CHARACTERS, Use character length for precision\n",
    "    )\n",
    "\n",
    "def chunk_documents(documents):\n",
    "    \"\"\"Chunk documents with our optimized settings - returns List\"\"\"\n",
    "    text_splitter = create_optimized_splitter()\n",
    "    chunks = text_splitter.split_documents(documents)  # This is List[Document]\n",
    "\n",
    "    print(\"\\nüî® Processing text chunks...\")\n",
    "    \n",
    "    print(f\"‚úÇÔ∏è Chunking Results:\")\n",
    "    print(f\"   Input documents: {len(documents)}\")\n",
    "    print(f\"   Output chunks: {len(chunks)}\")\n",
    "    print(f\"   Chunk type: {type(chunks[0])}\")  # Should show Document\n",
    "    \n",
    "    # Analyze chunk sizes - we're accessing .page_content of Document objects\n",
    "    chunk_sizes = [len(chunk.page_content) for chunk in chunks]\n",
    "    avg_size = sum(chunk_sizes) / len(chunk_sizes)\n",
    "    \n",
    "    print(f\"üìä Chunk Size Analysis:\")\n",
    "    print(f\"   Average: {avg_size:.0f} characters\")\n",
    "    print(f\"   Range: {min(chunk_sizes)} - {max(chunk_sizes)} characters\")\n",
    "    \n",
    "    # Add chunk-specific metadata to Document objects\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        chunk.metadata.update({\n",
    "            \"chunk_id\": i + 1,\n",
    "            \"chunk_size\": len(chunk.page_content),  # Accessing Document attribute\n",
    "            \"total_chunks\": len(chunks)\n",
    "        })\n",
    "    \n",
    "    return chunks  # Returns List[Document]\n",
    "\n",
    "# Process the chunks\n",
    "chunks = chunk_documents(documents)\n",
    "\n",
    "# Show chunk samples\n",
    "if chunks:\n",
    "    print(f\"\\nüìã Chunk Samples:\")\n",
    "    for i in range(min(3, len(chunks))):\n",
    "        print(f\"Chunk {i+1}: {chunks[i].page_content[:100]}...\")\n",
    "        print(f\"Size: {len(chunks[i].page_content)} chars | Category: {chunks[i].metadata.get('content_category', 'N/A')}\")\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf74877",
   "metadata": {},
   "source": [
    "## üìä Chunk Analysis\n",
    "\n",
    "*Examine the results of our chunking strategy*\n",
    "\n",
    "**What to check**:\n",
    "- Number of chunks created\n",
    "- Size distribution\n",
    "- Content quality\n",
    "\n",
    "**Common Issues**:\n",
    "- ‚ùå Chunks too small (lose context)\n",
    "- ‚ùå Chunks too large (irrelevant info)\n",
    "- ‚úÖ Balanced chunks (optimal retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "df5dba68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Chunk statistics:\n",
      "Min length: 7\n",
      "Max length: 799\n",
      "Avg length: 642.1\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Examine Chunk Distribution\n",
    "chunk_lengths = [len(chunk.page_content) for chunk in chunks]\n",
    "\n",
    "print(f\"üìä Chunk statistics:\")\n",
    "print(f\"Min length: {min(chunk_lengths)}\")\n",
    "print(f\"Max length: {max(chunk_lengths)}\")\n",
    "print(f\"Avg length: {sum(chunk_lengths)/len(chunk_lengths):.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "730ff077",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.documents.base.Document"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a88ed372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document_type': 'zdh_gdpr_handbook', 'document_name': 'ZDH_LEITFADEN_DATENSCHUTZ_BETRIEBE_HANDWERKER.pdf', 'language': 'german', 'source': '../2_data/raw/ZDH_LEITFADEN_DATENSCHUTZ_BETRIEBE_HANDWERKER.pdf', 'page_number': 2, 'total_pages': 99, 'content_length': 1299, 'content_category': 'legal_basis', 'section_type': 'content', 'creationdate': '2020-11-06T11:24:59+01:00', 'author': 'Kasper, Lisa', 'moddate': '2020-11-06T11:24:59+01:00', 'page': 1, 'page_label': '2', 'chunk_id': 2, 'chunk_size': 749, 'total_chunks': 266}\n"
     ]
    }
   ],
   "source": [
    "print(chunks[1].metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ede4b267",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Embedding Cost Calculation\n",
      "   Model: text-embedding-3-small\n",
      "   Texts: 266\n",
      "   Total tokens: 51,072\n",
      "   Cost: $0.001021\n",
      "   Avg tokens per text: 192.0\n",
      "\n",
      "üìà Project Cost Summary:\n",
      "   Total Document chunks: 266\n",
      "   Chunk object type: <class 'langchain_core.documents.base.Document'>\n",
      "   Estimated tokens: 51,072\n",
      "   Estimated cost: $0.0010\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Calculate Embedding Costs\n",
    "def analyze_chunk_costs(chunks):\n",
    "    \"\"\"Analyze costs for the chunked documents - chunks is List[Document]\"\"\"\n",
    "    # Extract just the text content for cost calculation\n",
    "    texts = [chunk.page_content for chunk in chunks]  # Convert to List[str]\n",
    "    cost_info = calculate_embedding_cost(texts)\n",
    "    \n",
    "    print(f\"\\nüìà Project Cost Summary:\")\n",
    "    print(f\"   Total Document chunks: {len(chunks)}\")\n",
    "    print(f\"   Chunk object type: {type(chunks[0])}\")\n",
    "    print(f\"   Estimated tokens: {cost_info['total_tokens']:,}\")\n",
    "    print(f\"   Estimated cost: ${cost_info['total_cost']:.4f}\")\n",
    "    \n",
    "    return cost_info\n",
    "\n",
    "chunk_costs = analyze_chunk_costs(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "11a03eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document_type': 'zdh_gdpr_handbook', 'document_name': 'ZDH_LEITFADEN_DATENSCHUTZ_BETRIEBE_HANDWERKER.pdf', 'language': 'german', 'source': '../2_data/raw/ZDH_LEITFADEN_DATENSCHUTZ_BETRIEBE_HANDWERKER.pdf', 'page_number': 1, 'total_pages': 99, 'content_length': 121, 'content_category': 'legal_basis', 'section_type': 'content', 'creationdate': '2020-11-06T11:24:59+01:00', 'author': 'Kasper, Lisa', 'moddate': '2020-11-06T11:24:59+01:00', 'page': 0, 'page_label': '1', 'chunk_id': 1, 'chunk_size': 121, 'total_chunks': 266}\n",
      "Leitfaden \n",
      "Datenschutzrecht \n",
      "Was Betriebe zu beachten haben \n",
      " \n",
      " \n",
      "Stand: November 2020 \n",
      " \n",
      "Abteilung Organisation und Recht\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0].metadata)\n",
    "print(chunks[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b185776e",
   "metadata": {},
   "source": [
    "## üíæ Save Results\n",
    "\n",
    "*Save processed chunks for the next notebook*\n",
    "\n",
    "**What we're saving**:\n",
    "- English text chunks with metadata\n",
    "- German text chunks with metadata  \n",
    "- Ready for embedding generation\n",
    "\n",
    "**Next Steps**:\n",
    "- Vector database setup in Notebook 2\n",
    "- Multilingual embedding generation\n",
    "- Cross-language search testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be403967",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Saved 266 chunks to ../2_data/processed/chunks.pkl\n",
      "üíæ Saved configuration to ../2_data/processed/config.pkl\n",
      "\n",
      "üéâ Notebook 1 Complete!\n",
      "==================================================\n",
      "üìù SUMMARY\n",
      "==================================================\n",
      "üìÑ PDF Pages Processed: 99\n",
      "‚úÇÔ∏è  Chunks Created: 266\n",
      "üí∞ Estimated Cost: $0.0009\n",
      "üî¢ Estimated Tokens: 46,778\n",
      "üìä Avg Chunk Size: 642 chars\n",
      "==================================================\n",
      "‚û°Ô∏è  Next: Run Notebook 2 to upload to Pinecone\n",
      "   Notebook 2 will require:\n",
      "   - OPENAI_API_KEY for embeddings\n",
      "   - PINECONE_API_KEY for vector database\n",
      "   - PINECONE_ENVIRONMENT for Pinecone setup\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Save Processed Data\n",
    "def save_processed_data(chunks):\n",
    "    \"\"\"Save processed chunks for next notebook\"\"\"\n",
    "    os.makedirs(\"../2_data/processed\", exist_ok=True)\n",
    "    \n",
    "    # Save chunks as serializable data\n",
    "    serializable_chunks = []\n",
    "    for chunk in chunks:\n",
    "        serializable_chunks.append({\n",
    "            'page_content': chunk.page_content,\n",
    "            'metadata': chunk.metadata\n",
    "        })\n",
    "    \n",
    "    with open(\"../2_data/processed/chunks.pkl\", \"wb\") as f:\n",
    "        pickle.dump(serializable_chunks, f)\n",
    "    \n",
    "    # Save configuration\n",
    "    config = {\n",
    "        \"index_name\": \"gdpr-compliance\",\n",
    "        \"total_chunks\": len(chunks),\n",
    "        \"chunk_size\": 800,\n",
    "        \"chunk_overlap\": 120,\n",
    "        \"embedding_model\": \"text-embedding-3-small\",\n",
    "        \"total_tokens\": cost_info['total_tokens'],\n",
    "        \"estimated_cost\": cost_info['total_cost']\n",
    "    }\n",
    "    \n",
    "    with open(\"../2_data/processed/config.pkl\", \"wb\") as f:\n",
    "        pickle.dump(config, f)\n",
    "    \n",
    "    print(f\"üíæ Saved {len(chunks)} chunks to ../2_data/processed/chunks.pkl\")\n",
    "    print(f\"üíæ Saved configuration to ../2_data/processed/config.pkl\")\n",
    "    \n",
    "    return config\n",
    "\n",
    "# Save everything\n",
    "if chunks:\n",
    "    config = save_processed_data(chunks)\n",
    "\n",
    "print(\"\\nüéâ Notebook 1 Complete!\")\n",
    "print(\"=\"*50)\n",
    "print(\"üìù SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"üìÑ PDF Pages Processed: {len(documents)}\")\n",
    "print(f\"‚úÇÔ∏è  Chunks Created: {len(chunks)}\")\n",
    "print(f\"üí∞ Estimated Cost: ${cost_info['total_cost']:.4f}\")\n",
    "print(f\"üî¢ Estimated Tokens: {cost_info['total_tokens']:,}\")\n",
    "print(f\"üìä Avg Chunk Size: {sum(len(c.page_content) for c in chunks)/len(chunks):.0f} chars\")\n",
    "print(\"=\"*50)\n",
    "print(\"‚û°Ô∏è  Next: Run Notebook 2 to upload to Pinecone\")\n",
    "print(\"   Notebook 2 will require:\")\n",
    "print(\"   - OPENAI_API_KEY for embeddings\")\n",
    "print(\"   - PINECONE_API_KEY for vector database\")\n",
    "print(\"   - PINECONE_ENVIRONMENT for Pinecone setup\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "481b9fff",
   "metadata": {},
   "source": [
    "-----------\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cddba68a",
   "metadata": {},
   "source": [
    "-----\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f93de0",
   "metadata": {},
   "source": [
    "# DRAFTs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d20803c",
   "metadata": {},
   "source": [
    "## üîç Load & Explore Data\n",
    "\n",
    "*Load our sample data and examine its structure*\n",
    "\n",
    "**Key Questions**:\n",
    "- How much text do we have?\n",
    "- What's the content structure?\n",
    "- Are there clear sections we can use?\n",
    "\n",
    "*Understanding your data is crucial for good chunking strategy*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LangChain",
   "language": "python",
   "name": "langchain_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
