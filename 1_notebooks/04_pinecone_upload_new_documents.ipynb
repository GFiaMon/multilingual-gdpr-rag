{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üóÑÔ∏è GDPR Compliance Agent - Notebook 4: Reusable Pinecone Upload for New Documents\n",
        "\n",
        "This notebook modularizes the pipeline to process new GDPR-related PDFs and upload them to Pinecone. It combines and generalizes the workflows from `01_text_pdf_processing_document-2.ipynb` and `02_pinecone_embeddings-2.ipynb` for future reuse.\n",
        "\n",
        "- **What it does**:\n",
        "  - Extracts text and metadata from one or more PDFs\n",
        "  - Chunks text using optimized parameters for legal documents\n",
        "  - Optionally estimates embedding cost\n",
        "  - Uploads chunks to Pinecone with OpenAI embeddings\n",
        "  - Provides a simple retrieval verification\n",
        "\n",
        "- **How to use**:\n",
        "  - Put your PDFs in `2_data/raw/`\n",
        "  - Set `PDF_PATHS` in the config cell below (single or multiple files)\n",
        "  - Ensure `.env` contains `OPENAI_API_KEY` and `PINECONE_API_KEY`\n",
        "  - Run cells from top to bottom\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Libraries imported successfully!\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/guillermo/venvs/langchain_venv/lib/python3.11/site-packages/langchain_pinecone/__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
          ]
        }
      ],
      "source": [
        "# Cell 1: Setup and Imports\n",
        "import os\n",
        "import sys\n",
        "import time\n",
        "import pickle\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "\n",
        "# Add project root to Python path\n",
        "sys.path.append(os.path.abspath('..'))\n",
        "\n",
        "# Third-party libraries\n",
        "from dotenv import load_dotenv\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.schema import Document\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "\n",
        "\n",
        "# Helper utilities\n",
        "from src.embedding_cost_calculator import calculate_embedding_cost, quick_cost\n",
        "\n",
        "print(\"‚úÖ Libraries imported successfully!\")\n",
        "\n",
        "\n",
        "def init_environment() -> Tuple[Optional[str], Optional[str]]:\n",
        "    \"\"\"Load environment variables once and return keys.\"\"\"\n",
        "    load_dotenv()\n",
        "    openai_key = os.getenv('OPENAI_API_KEY')\n",
        "    pinecone_key = os.getenv('PINECONE_API_KEY')\n",
        "    return openai_key, pinecone_key\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 2: Environment and Configuration\n",
        "OPENAI_API_KEY, PINECONE_API_KEY = init_environment()\n",
        "\n",
        "# User-configurable parameters\n",
        "SINGLE_PDF_PATH: str = \"../2_data/raw/bitkom-leitfaden-kuenstliche-intelligenz-und-datenschutz-auflage-2.pdf\"  # Select ONE specific PDF\n",
        "DOCUMENT_TYPE: str = \"bitkom_ai_gdpr_handbook\"  # Set the metadata category for this document\n",
        "\n",
        "INDEX_NAME: str = \"gdpr-compliance-openai\"  # Reuse existing or create new\n",
        "EMBEDDING_MODEL: str = \"text-embedding-3-small\"\n",
        "CHUNK_SIZE: int = 800\n",
        "CHUNK_OVERLAP: int = 120\n",
        "\n",
        "# Preview-first flags\n",
        "PREVIEW_ONLY: bool = True   # If True, process and preview chunks; skip upload\n",
        "PREVIEW_NUM_CHUNKS: int = 5 # Show this many chunks in preview\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîë API keys configured via init_environment()\n",
            "üìÅ Using Pinecone index: gdpr-compliance-openai\n"
          ]
        }
      ],
      "source": [
        "# Configure environment (single initialization already done)\n",
        "# Keys are set via init_environment() in Cell 1; avoid prompting or re-setting.\n",
        "print(\"üîë API keys configured via init_environment()\")\n",
        "print(f\"üìÅ Using Pinecone index: {INDEX_NAME}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîë Environment Configuration:\n",
            "   OpenAI API Key: ‚úÖ\n",
            "   Pinecone API Key: ‚úÖ\n",
            "üß© Config:\n",
            "   SINGLE_PDF_PATH: ../2_data/raw/bitkom-leitfaden-kuenstliche-intelligenz-und-datenschutz-auflage-2.pdf\n",
            "   DOCUMENT_TYPE: bitkom_ai_gdpr_handbook\n",
            "   INDEX_NAME: gdpr-compliance-openai\n",
            "   MODEL: text-embedding-3-small\n",
            "   CHUNK_SIZE/OVERLAP: 800/120\n",
            "   PREVIEW_ONLY: True\n",
            "   PREVIEW_NUM_CHUNKS: 5\n"
          ]
        }
      ],
      "source": [
        "# Verify environment variables and print config\n",
        "print(\"üîë Environment Configuration:\")\n",
        "print(f\"   OpenAI API Key: {'‚úÖ' if OPENAI_API_KEY else '‚ùå'}\")\n",
        "print(f\"   Pinecone API Key: {'‚úÖ' if PINECONE_API_KEY else '‚ùå'}\")\n",
        "print(\"üß© Config:\")\n",
        "print(f\"   SINGLE_PDF_PATH: {SINGLE_PDF_PATH}\")\n",
        "print(f\"   DOCUMENT_TYPE: {DOCUMENT_TYPE}\")\n",
        "print(f\"   INDEX_NAME: {INDEX_NAME}\")\n",
        "print(f\"   MODEL: {EMBEDDING_MODEL}\")\n",
        "print(f\"   CHUNK_SIZE/OVERLAP: {CHUNK_SIZE}/{CHUNK_OVERLAP}\")\n",
        "print(f\"   PREVIEW_ONLY: {PREVIEW_ONLY}\")\n",
        "print(f\"   PREVIEW_NUM_CHUNKS: {PREVIEW_NUM_CHUNKS}\")\n",
        "\n",
        "if not all([OPENAI_API_KEY, PINECONE_API_KEY]):\n",
        "    print(\"‚ö†Ô∏è  Missing environment variables! Please set in your .env file.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 3: PDF Extraction Utilities\n",
        "\n",
        "def categorize_content(text: str) -> str:\n",
        "    text_lower = text.lower()\n",
        "    if any(k in text_lower for k in ['kunde', 'customer', 'marketing']):\n",
        "        return \"customer_data\"\n",
        "    if any(k in text_lower for k in ['mitarbeiter', 'employee', 'personal']):\n",
        "        return \"employee_data\"\n",
        "    if any(k in text_lower for k in ['recht', 'law', 'gesetz', 'dsgvo']):\n",
        "        return \"legal_basis\"\n",
        "    if any(k in text_lower for k in ['sicherheit', 'security', 'datenschutzverletzung']):\n",
        "        return \"security\"\n",
        "    if any(k in text_lower for k in ['speicherung', 'retention', 'aufbewahrung']):\n",
        "        return \"data_retention\"\n",
        "    return \"general\"\n",
        "\n",
        "\n",
        "def identify_section_type(text: str) -> str:\n",
        "    text = text.strip()\n",
        "    if len(text) < 200 and any(ind in text for ind in ['KAPITEL', 'ARTIKEL', 'SECTION']):\n",
        "        return \"section_header\"\n",
        "    if len(text) < 100 and text.isupper():\n",
        "        return \"heading\"\n",
        "    return \"content\"\n",
        "\n",
        "\n",
        "def extract_pdf_with_metadata(pdf_path: str) -> List[Document]:\n",
        "    print(f\"üìÑ Extracting from: {pdf_path}\")\n",
        "    if not os.path.exists(pdf_path):\n",
        "        print(f\"‚ùå File not found: {pdf_path}\")\n",
        "        return []\n",
        "\n",
        "    loader = PyPDFLoader(pdf_path)\n",
        "    docs = loader.load()\n",
        "    if not docs:\n",
        "        print(\"‚ùå No pages extracted from PDF.\")\n",
        "        return []\n",
        "\n",
        "    print(f\"‚úÖ Successfully extracted {len(docs)} pages\")\n",
        "\n",
        "    enhanced_docs: List[Document] = []\n",
        "    for i, doc in enumerate(docs):\n",
        "        keys_to_remove = ['producer', 'creator']\n",
        "        clean_metadata: Dict[str, str] = {}\n",
        "        for key, value in doc.metadata.items():\n",
        "            if key not in keys_to_remove:\n",
        "                clean_metadata[key] = value\n",
        "\n",
        "        custom_metadata = {\n",
        "            \"document_type\": DOCUMENT_TYPE,\n",
        "            \"document_name\": os.path.basename(pdf_path),\n",
        "            \"language\": \"german\",\n",
        "            \"source\": pdf_path,\n",
        "            \"page_number\": i + 1,\n",
        "            \"total_pages\": len(docs),\n",
        "            \"content_length\": len(doc.page_content),\n",
        "            \"content_category\": categorize_content(doc.page_content),\n",
        "            \"section_type\": identify_section_type(doc.page_content),\n",
        "        }\n",
        "        final_metadata = {**custom_metadata, **clean_metadata}\n",
        "        enhanced_docs.append(Document(page_content=doc.page_content, metadata=final_metadata))\n",
        "\n",
        "    if enhanced_docs:\n",
        "        print(\"\\nüìã First page sample:\")\n",
        "        print(enhanced_docs[0].page_content[:200] + \"...\")\n",
        "        print(f\"üìä Metadata: {enhanced_docs[0].metadata}\")\n",
        "\n",
        "    return enhanced_docs\n",
        "\n",
        "\n",
        "def extract_single_pdf(pdf_path: str) -> List[Document]:\n",
        "    return extract_pdf_with_metadata(pdf_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4: Chunking and Cost Utilities\n",
        "\n",
        "def create_optimized_splitter(chunk_size: int = CHUNK_SIZE, chunk_overlap: int = CHUNK_OVERLAP) -> RecursiveCharacterTextSplitter:\n",
        "    return RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap,\n",
        "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \"; \", \", \", \" \", \"\"],\n",
        "        length_function=len,\n",
        "    )\n",
        "\n",
        "\n",
        "def chunk_documents(documents: List[Document]) -> List[Document]:\n",
        "    splitter = create_optimized_splitter()\n",
        "    chunks = splitter.split_documents(documents)\n",
        "\n",
        "    # annotate chunks\n",
        "    for i, chunk in enumerate(chunks):\n",
        "        chunk.metadata.update({\n",
        "            \"chunk_id\": i + 1,\n",
        "            \"chunk_size\": len(chunk.page_content),\n",
        "            \"total_chunks\": len(chunks),\n",
        "        })\n",
        "\n",
        "    print(\"‚úÇÔ∏è Chunking Results:\")\n",
        "    print(f\"   Input documents: {len(documents)}\")\n",
        "    print(f\"   Output chunks: {len(chunks)}\")\n",
        "\n",
        "    return chunks\n",
        "\n",
        "\n",
        "def estimate_embedding_cost_for_chunks(chunks: List[Document]) -> Dict[str, float]:\n",
        "    texts = [c.page_content for c in chunks]\n",
        "    return calculate_embedding_cost(texts)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5: Pinecone Initialization and Upload Utilities\n",
        "\n",
        "# ---------------------------\n",
        "# Pinecone Initialization\n",
        "# ---------------------------\n",
        "def init_pinecone(api_key: str, index_name: str = \"gdpr-compliance-openai\", environment: str = \"us-east-1\"):\n",
        "    \"\"\"Initialize Pinecone connection using guard clauses only.\"\"\"\n",
        "    if not api_key:\n",
        "        print(\"‚ùå PINECONE_API_KEY is missing!\")\n",
        "        return None, None\n",
        "\n",
        "    print(\"üîå Initializing Pinecone...\")\n",
        "    pc = Pinecone(api_key=api_key)\n",
        "    print(\"‚úÖ Pinecone initialized successfully\")\n",
        "\n",
        "    names = pc.list_indexes().names()\n",
        "    if index_name in names:\n",
        "        print(f\"‚úÖ Index '{index_name}' exists\")\n",
        "        status = pc.describe_index(index_name).status\n",
        "        if hasattr(status, 'ready') and not status.ready:\n",
        "            print(\"‚è≥ Waiting for index to be ready...\")\n",
        "            while not pc.describe_index(index_name).status.ready:\n",
        "                time.sleep(1)\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Index '{index_name}' not found.\")\n",
        "\n",
        "    index = pc.Index(index_name) if index_name in names else None\n",
        "    return pc, index\n",
        "\n",
        "def create_vectorstore_and_upload(chunks: List[Document], index_name: str = INDEX_NAME, embedding_model: str = EMBEDDING_MODEL):\n",
        "    print(\"üîÑ Creating Pinecone vector store...\")\n",
        "    embeddings = OpenAIEmbeddings(model=embedding_model)\n",
        "    vectorstore = PineconeVectorStore.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        index_name=index_name\n",
        "    )\n",
        "    print(f\"‚úÖ Successfully loaded {len(chunks)} documents into Pinecone\")\n",
        "    total_chars = sum(len(c.page_content) for c in chunks)\n",
        "    print(f\"üìä Stats: {len(chunks)} chunks, ~{total_chars} characters\")\n",
        "    return vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6: Main Runner - Process and Optional Upload\n",
        "\n",
        "def process_and_optionally_upload(single_pdf_path: str, preview_only: bool = PREVIEW_ONLY) -> Tuple[List[Document], List[Document], Optional[PineconeVectorStore]]:\n",
        "    # 1) Extract one PDF\n",
        "    docs = extract_single_pdf(single_pdf_path)\n",
        "    if not docs:\n",
        "        print(\"‚ùå No documents extracted. Exiting.\")\n",
        "        return [], [], None\n",
        "\n",
        "    # 2) Chunk\n",
        "    chunks = chunk_documents(docs)\n",
        "\n",
        "    # 3) Cost estimation (optional)\n",
        "    cost_usd = estimate_embedding_cost_for_chunks(chunks)\n",
        "    if isinstance(cost_usd, (int, float)):\n",
        "        print(\"\\nüìä Embedding Cost (estimate)\")\n",
        "        print(f\"   Model: {EMBEDDING_MODEL}\")\n",
        "        print(f\"   Texts: {len(chunks)}\")\n",
        "        print(f\"   Estimated cost: ${cost_usd:.6f}\")\n",
        "\n",
        "    # 4) Preview-first guard\n",
        "    if preview_only:\n",
        "        print(\"\\nüõë PREVIEW_ONLY=True -> Skipping Pinecone upload. Inspect chunks below.\")\n",
        "        return docs, chunks, None\n",
        "\n",
        "    # 5) Pinecone upload (when preview_only=False)\n",
        "    pc, index = init_pinecone(PINECONE_API_KEY, INDEX_NAME)\n",
        "    if not index:\n",
        "        print(\"‚ùå Pinecone not available or index missing. Exiting before upload.\")\n",
        "        return docs, chunks, None\n",
        "\n",
        "    vectorstore = create_vectorstore_and_upload(chunks, INDEX_NAME, EMBEDDING_MODEL)\n",
        "    return docs, chunks, vectorstore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Extracting from: ../2_data/raw/bitkom-leitfaden-kuenstliche-intelligenz-und-datenschutz-auflage-2.pdf\n",
            "‚úÖ Successfully extracted 57 pages\n",
            "\n",
            "üìã First page sample:\n",
            "K√ºnstliche \n",
            "Intelligenz & \n",
            "Datenschutz \n",
            "Praxisleitfaden Version 2.0 | Neuauflage...\n",
            "üìä Metadata: {'document_type': 'bitkom_ai_gdpr_handbook', 'document_name': 'bitkom-leitfaden-kuenstliche-intelligenz-und-datenschutz-auflage-2.pdf', 'language': 'german', 'source': '../2_data/raw/bitkom-leitfaden-kuenstliche-intelligenz-und-datenschutz-auflage-2.pdf', 'page_number': 1, 'total_pages': 57, 'content_length': 80, 'content_category': 'general', 'section_type': 'content', 'creationdate': '2025-08-12T09:15:52+02:00', 'title': 'Praxisleitfaden KI & Datenschutz', 'author': 'Isabelle Stroot', 'subject': 'KI-Datenschutz', 'moddate': '2025-08-12T09:15:52+02:00', 'page': 0, 'page_label': '1'}\n",
            "‚úÇÔ∏è Chunking Results:\n",
            "   Input documents: 57\n",
            "   Output chunks: 249\n",
            "üìä Cost Calculation:\n",
            "   - Number of texts: 249\n",
            "   - Total tokens: 51120\n",
            "   - Model: text-embedding-3-small\n",
            "\n",
            "üìä Embedding Cost (estimate)\n",
            "   Model: text-embedding-3-small\n",
            "   Texts: 249\n",
            "   Estimated cost: $0.005112\n",
            "\n",
            "üõë PREVIEW_ONLY=True -> Skipping Pinecone upload. Inspect chunks below.\n"
          ]
        }
      ],
      "source": [
        "# Run the pipeline in preview mode by default\n",
        "PREVIEW_ONLY = True\n",
        "DOCS, CHUNKS, VECTORSTORE = process_and_optionally_upload(SINGLE_PDF_PATH, PREVIEW_ONLY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìã Showing first 5 chunk(s):\n",
            "\n",
            "--- Chunk 1 ---\n",
            "Meta: {'document_name': 'bitkom-leitfaden-kuenstliche-intelligenz-und-datenschutz-auflage-2.pdf', 'page_number': 1, 'content_category': 'general', 'chunk_id': 1, 'chunk_size': 80}\n",
            "K√ºnstliche \n",
            "Intelligenz & \n",
            "Datenschutz \n",
            "Praxisleitfaden Version 2.0 | Neuauflage\n",
            "\n",
            "--- Chunk 2 ---\n",
            "Meta: {'document_name': 'bitkom-leitfaden-kuenstliche-intelligenz-und-datenschutz-auflage-2.pdf', 'page_number': 2, 'content_category': 'legal_basis', 'chunk_id': 2, 'chunk_size': 789}\n",
            "Praxisleitfaden KI & Datenschutz \n",
            "2 \n",
            "Inhalt \n",
            "Geleitwort 4 \n",
            "1 Ziel des Leitfadens 5 \n",
            "Wann sprechen wir √ºberhaupt von K√ºnstlicher Intelligenz? 6 \n",
            "Ethischer Rahmen: Vertrauensw√ºrdige KI-Gestaltung strategisch \n",
            "verankern und umsetzen 8 \n",
            "Rechtsrahmen beim Einsatz von KI 9 \n",
            "2 Checkliste zum datenschutzkonformen Einsatz von KI 12 \n",
            "Training eigener KI-Modelle und Systeme 13 \n",
            "Nutzung von KI-Systemen und Mo...\n",
            "\n",
            "--- Chunk 3 ---\n",
            "Meta: {'document_name': 'bitkom-leitfaden-kuenstliche-intelligenz-und-datenschutz-auflage-2.pdf', 'page_number': 2, 'content_category': 'legal_basis', 'chunk_id': 3, 'chunk_size': 691}\n",
            "Zweckbindung, Art. 5 Abs.1lit.b DS-GVO 22 \n",
            "Datenminimierung, Art. 5 Abs.1lit.c DS-GVO 23 \n",
            "Richtigkeit, Art. 5 Abs.1lit.d DS-GVO 23 \n",
            "Rechenschaftspflicht, Art. 5 Abs.2 DS-GVO 23 \n",
            "Rechtsgrundlage bei der Nutzung von personenbezogenen Daten zu \n",
            "Trainingszwecken 24 \n",
            "Berechtigtes Interesse 25 \n",
            "KI-Training mit Daten aus √∂ffentlichen Nutzerprofilen 26 \n",
            "Einwilligung 28 \n",
            "Vertragserf√ºllung 28 \n",
            "Rechtliche Ve...\n",
            "\n",
            "--- Chunk 4 ---\n",
            "Meta: {'document_name': 'bitkom-leitfaden-kuenstliche-intelligenz-und-datenschutz-auflage-2.pdf', 'page_number': 3, 'content_category': 'legal_basis', 'chunk_id': 4, 'chunk_size': 791}\n",
            "Praxisleitfaden KI & Datenschutz \n",
            "3 \n",
            "Artikel 9 DS-GVO: Verarbeitung besonderer Kategorien \n",
            "personenbezogener Daten 30 \n",
            "Folgen f√ºr den Einsatz von KI-Modellen bei  DS-GVO-widrigem KI-\n",
            "Training 32 \n",
            "Die rechtliche Einsch√§tzung des Europ√§ischen Datenschutzausschusses 33 \n",
            "Transparenz und Informationspflichten 35 \n",
            "Weitere Betroffenenrechte (Art. 12,15 ff.  DS-GVO) Artikel 12 ff. DS-GVO: \n",
            "Umsetzung  von ...\n",
            "\n",
            "--- Chunk 5 ---\n",
            "Meta: {'document_name': 'bitkom-leitfaden-kuenstliche-intelligenz-und-datenschutz-auflage-2.pdf', 'page_number': 3, 'content_category': 'legal_basis', 'chunk_id': 5, 'chunk_size': 751}\n",
            "Alleinige Verantwortlichkeit (Independent Controller) 42 \n",
            "Gemeinsame Verantwortlichkeit (Joint Controller) 42 \n",
            "Auftragsverarbeitung (Data Processor) 43 \n",
            "Artikel 25 ff. DS-GVO: Privacy by Design/ Privacy by Default und Einsatz \n",
            "von geeigneten technischen und organisatorischen Ma√ünahmen 44 \n",
            "Artikel 30 DS-GVO: Aufnahme der Verarbeitung in das Verzeichnis von \n",
            "Verarbeitungst√§tigkeiten 46 \n",
            "Artikel 33, ...\n",
            "(Skipping retrieval test ‚Äî no upload performed in PREVIEW_ONLY mode)\n"
          ]
        }
      ],
      "source": [
        "# Cell 7: Preview - Inspect First Chunks Before Upload\n",
        "def preview_chunks(chunks: List[Document], num: int = PREVIEW_NUM_CHUNKS):\n",
        "    if not chunks:\n",
        "        print(\"‚ö†Ô∏è  No chunks to preview.\")\n",
        "        return\n",
        "    print(f\"üìã Showing first {min(num, len(chunks))} chunk(s):\")\n",
        "    for i in range(min(num, len(chunks))):\n",
        "        c = chunks[i]\n",
        "        meta = {k: c.metadata.get(k) for k in [\"document_name\", \"page_number\", \"content_category\", \"chunk_id\", \"chunk_size\"]}\n",
        "        print(f\"\\n--- Chunk {i+1} ---\")\n",
        "        print(f\"Meta: {meta}\")\n",
        "        print(c.page_content[:400] + (\"...\" if len(c.page_content) > 400 else \"\"))\n",
        "\n",
        "\n",
        "# Run chunk preview\n",
        "preview_chunks(CHUNKS, PREVIEW_NUM_CHUNKS)\n",
        "\n",
        "# Optional: similarity search only runs after upload\n",
        "def preview_search(vectorstore: Optional[PineconeVectorStore], queries: List[str], k: int = 2):\n",
        "    if vectorstore is None:\n",
        "        print(\"(Skipping retrieval test ‚Äî no upload performed in PREVIEW_ONLY mode)\")\n",
        "        return\n",
        "    print(\"üß™ Testing Vector Store Retrieval...\")\n",
        "    for query in queries:\n",
        "        print(f\"\\nüîç Query: '{query}'\")\n",
        "        results = vectorstore.similarity_search(query, k=k)\n",
        "        print(f\"   Found {len(results)} relevant chunks:\")\n",
        "        for i, doc in enumerate(results):\n",
        "            print(f\"   {i+1}. {doc.page_content[:150]}...\")\n",
        "        print(\"   \" + \"‚îÄ\" * 50)\n",
        "\n",
        "TEST_QUERIES = [\n",
        "    \"Was ist die Datenschutzrichtlinie?\",\n",
        "    \"Wie sollen Kundendaten behandelt werden?\",\n",
        "]\n",
        "\n",
        "preview_search(VECTORSTORE, TEST_QUERIES, k=2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÑ Extracting from: ../2_data/raw/bitkom-leitfaden-kuenstliche-intelligenz-und-datenschutz-auflage-2.pdf\n",
            "‚úÖ Successfully extracted 57 pages\n",
            "\n",
            "üìã First page sample:\n",
            "K√ºnstliche \n",
            "Intelligenz & \n",
            "Datenschutz \n",
            "Praxisleitfaden Version 2.0 | Neuauflage...\n",
            "üìä Metadata: {'document_type': 'bitkom_ai_gdpr_handbook', 'document_name': 'bitkom-leitfaden-kuenstliche-intelligenz-und-datenschutz-auflage-2.pdf', 'language': 'german', 'source': '../2_data/raw/bitkom-leitfaden-kuenstliche-intelligenz-und-datenschutz-auflage-2.pdf', 'page_number': 1, 'total_pages': 57, 'content_length': 80, 'content_category': 'general', 'section_type': 'content', 'creationdate': '2025-08-12T09:15:52+02:00', 'title': 'Praxisleitfaden KI & Datenschutz', 'author': 'Isabelle Stroot', 'subject': 'KI-Datenschutz', 'moddate': '2025-08-12T09:15:52+02:00', 'page': 0, 'page_label': '1'}\n",
            "‚úÇÔ∏è Chunking Results:\n",
            "   Input documents: 57\n",
            "   Output chunks: 249\n",
            "üìä Cost Calculation:\n",
            "   - Number of texts: 249\n",
            "   - Total tokens: 51120\n",
            "   - Model: text-embedding-3-small\n",
            "\n",
            "üìä Embedding Cost (estimate)\n",
            "   Model: text-embedding-3-small\n",
            "   Texts: 249\n",
            "   Estimated cost: $0.005112\n",
            "üîå Initializing Pinecone...\n",
            "‚úÖ Pinecone initialized successfully\n",
            "‚úÖ Index 'gdpr-compliance-openai' exists\n",
            "üîÑ Creating Pinecone vector store...\n",
            "‚úÖ Successfully loaded 249 documents into Pinecone\n",
            "üìä Stats: 249 chunks, ~170563 characters\n"
          ]
        }
      ],
      "source": [
        "# Cell 8: Upload to Pinecone (Run after verifying chunks)\n",
        "\n",
        "# Set preview flag to False to enable upload\n",
        "PREVIEW_ONLY = False\n",
        "\n",
        "# Run end-to-end with upload\n",
        "DOCS, CHUNKS, VECTORSTORE = process_and_optionally_upload(SINGLE_PDF_PATH, PREVIEW_ONLY)\n",
        "\n",
        "# Optional: quick retrieval sanity check after upload\n",
        "# preview_search(VECTORSTORE, TEST_QUERIES, k=2)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langchain_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
